import torch
import torch.nn as nn
import json
import time
import os
import math

# --- 1. å®éªŒè®°å½•å·¥å…· ---
class ExperimentLogger:
    def __init__(self, exp_dir):
        self.log_path = os.path.join(exp_dir, 'metrics.json')
        self.metrics = {
            'train_loss': [], 'val_loss': [], 
            'epoch_times': [], 'bleu_scores': [],
            'config': {}
        }
    
    def log(self, epoch, train_loss, val_loss, time_taken):
        self.metrics['train_loss'].append(train_loss)
        self.metrics['val_loss'].append(val_loss)
        self.metrics['epoch_times'].append(time_taken)
        self.save()

    def save(self):
        with open(self.log_path, 'w') as f:
            json.dump(self.metrics, f, indent=4)

# --- 2. æ¶æ„æ¶ˆèç»„ä»¶: RMSNorm ---
class RMSNorm(nn.Module):
    def __init__(self, dim, eps=1e-8):
        super().__init__()
        self.scale = dim ** -0.5
        self.eps = eps
        self.g = nn.Parameter(torch.ones(dim))

    def forward(self, x):
        norm = torch.norm(x, dim=-1, keepdim=True) * self.scale
        return x / (norm + self.eps) * self.g

# --- 3. è§£ç ç­–ç•¥: Beam Search ---
def beam_search_decode(model, src, src_vocab, tgt_vocab, device, beam_width=3, max_len=50, model_type='transformer'):
    """
    é€šç”¨ Beam Search å®ç°ï¼Œæ”¯æŒ RNN å’Œ Transformer
    """
    model.eval()
    sos_idx = tgt_vocab['<sos>']
    eos_idx = tgt_vocab['<eos>']
    pad_idx = 0 # å‡è®¾ <pad> index ä¸º 0
    
    # Encoder é˜¶æ®µ
    with torch.no_grad():
        src_tensor = src.unsqueeze(0).to(device) # [1, Seq]
        # ğŸŸ¢ ä¿®å¤ç‚¹ 1: ç”Ÿæˆ src_padding_mask
        src_padding_mask = (src_tensor == pad_idx).to(device)
        
        if model_type == 'rnn':
            enc_out, hidden = model.encoder(src_tensor)
            # RNN Decoder åˆå§‹çŠ¶æ€
            # hidden: [Layers, Batch, Hid]
            dec_input = torch.tensor([[sos_idx]], device=device)
            # çŠ¶æ€å…ƒç»„: (log_prob, sequence, decoder_hidden)
            candidates = [(0.0, [sos_idx], hidden)]
        
        elif model_type == 'transformer':
            # Transformer Encoder
            memory = model.encode(src_tensor)
            # çŠ¶æ€å…ƒç»„: (log_prob, sequence) - Transformer æ— éœ€ä¼ é€’ hidden
            candidates = [(0.0, [sos_idx])]
            
    final_candidates = []
    
    for _ in range(max_len):
        new_candidates = []
        for score, seq, *args in candidates:
            if seq[-1] == eos_idx:
                final_candidates.append((score, seq, *args))
                continue
            
            # å‡†å¤‡ Decoder è¾“å…¥
            if model_type == 'rnn':
                hidden_state = args[0]
                dec_input = torch.tensor([[seq[-1]]], device=device)
                
                # RNN å•æ­¥è§£ç 
                logits, new_hidden = model.decode_step(dec_input, hidden_state, enc_out)
                log_probs = torch.log_softmax(logits, dim=-1).squeeze(0) # [Vocab]
            
            elif model_type == 'transformer':
                tgt_tensor = torch.tensor([seq], device=device)
                # ğŸŸ¢ ä¿®å¤ç‚¹ 2: ä¼ å…¥ src_padding_mask
                logits = model.decode(tgt_tensor, memory, src_padding_mask)
                log_probs = torch.log_softmax(logits[:, -1, :], dim=-1).squeeze(0)
            
            # é€‰å– Top-K
            topk_probs, topk_ids = torch.topk(log_probs, beam_width)
            
            for k in range(beam_width):
                new_score = score + topk_probs[k].item()
                new_seq = seq + [topk_ids[k].item()]
                
                if model_type == 'rnn':
                    new_candidates.append((new_score, new_seq, new_hidden))
                else:
                    new_candidates.append((new_score, new_seq))
        
        # æ’åºå¹¶æˆªå– Beam Width
        ordered = sorted(new_candidates, key=lambda x: x[0], reverse=True)
        candidates = ordered[:beam_width]
        
        if len(candidates) == 0: break # æ‰€æœ‰éƒ½ç»“æŸäº†
        
    # å¦‚æœæ²¡æœ‰ç”Ÿæˆå®Œï¼Œå–å½“å‰çš„æœ€ä½³
    if not final_candidates:
        final_candidates = candidates
        
    best_score, best_seq, *_ = sorted(final_candidates, key=lambda x: x[0], reverse=True)[0]
    
    # è½¬å›æ–‡æœ¬
    tokens = [tgt_vocab.itos[i] for i in best_seq[1:] if i != eos_idx] # å»æ‰ SOS/EOS
    return " ".join(tokens)